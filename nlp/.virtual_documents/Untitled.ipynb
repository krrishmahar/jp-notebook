





import nltk


corpus = '''Hello, I'm krrish mahar, I'm 19 yrs old.
'''
print(corpus)


# Tokenization
## Sentence -> paragraph
from nltk.tokenize import sent_tokenize
document = sent_tokenize(corpus)
sent_tokenize(corpus)


for sentence in document:
    print(sentence)


## Tokenization
## Paragraph -> words
## sentence -> words

from nltk.tokenize import word_tokenize
word_tokenize(corpus)


from nltk.tokenize import wordpunct_tokenize
wordpunct_tokenize(corpus)


from nltk.tokenize import TreebankWordTokenizer
TreebankWordTokenizer().tokenize(corpus)





from nltk.stem import PorterStemmer
stemmer = PorterStemmer()
stemmer.stem('coding')


from nltk.stem import RegexpStemmer
reg_Stemmer = RegexpStemmer('ing$|s$|e$|able$', min=4)

reg_Stemmer.stem('bathing')


reg_Stemmer.stem('ingeating')





from nltk.stem import SnowballStemmer
snowball = SnowballStemmer('english')


print(stemmer.stem('fairly'), stemmer.stem('supportingly'))

print(snowball.stem('fairly'), snowball.stem('supportingly'))


# But Stemming still fails on some words, so we use Lemmetization
snowball.stem('goes')





from nltk.stem import WordNetLemmatizer

lemmatizer = WordNetLemmatizer()


# POS TAG
'''POS- Noun-n
verb-v
adjective-a
adverb-r'''
lemmatizer.lemmatize("goes", pos=)


words = ["better","walking","running","gone","am,", "are," ,"mice","studies","ate"]
for word in words:
    print(word+"----->"+lemmatizer.lemmatize(word, pos='v'))



