{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbd622b2-96c9-4344-ada4-945be6db67ef",
   "metadata": {},
   "source": [
    "## NLP\n",
    "basic terminology - \n",
    "* corpus - paragraph\n",
    "* document - sentence\n",
    "* vocalbury - unique words\n",
    "* words - sentences/words\n",
    "* tokenization - process of converting paragraph/sentence to tokens or sentence -> words"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7dba2d4e-5a22-4b8e-b458-0dec638fe79a",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "!conda install -c conda-forge nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96e90da3-80e3-485d-892b-adf36eee2e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd8e01dd-a763-402f-88bc-e036468924cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, I'm krrish mahar, I'm 19 yrs old.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "corpus = '''Hello, I'm krrish mahar, I'm 19 yrs old.\n",
    "'''\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "29cbbeed-5652-4e7e-b5c2-3e18794db0ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Hello, I'm krrish mahar, I'm 19 yrs old.\"]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenization\n",
    "## Sentence -> paragraph\n",
    "from nltk.tokenize import sent_tokenize\n",
    "document = sent_tokenize(corpus)\n",
    "sent_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "311ad99a-a136-4304-b692-5f27cfd2be08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, I'm krrish mahar, I'm 19 yrs old.\n"
     ]
    }
   ],
   "source": [
    "for sentence in document:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0874128c-f9d2-4ef2-bb48-fbfe07a4761f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " ',',\n",
       " 'I',\n",
       " \"'m\",\n",
       " 'krrish',\n",
       " 'mahar',\n",
       " ',',\n",
       " 'I',\n",
       " \"'m\",\n",
       " '19',\n",
       " 'yrs',\n",
       " 'old',\n",
       " '.']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Tokenization\n",
    "## Paragraph -> words\n",
    "## sentence -> words\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "word_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9d81d23d-fa26-4e22-a705-9de2e42ba690",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " ',',\n",
       " 'I',\n",
       " \"'\",\n",
       " 'm',\n",
       " 'krrish',\n",
       " 'mahar',\n",
       " ',',\n",
       " 'I',\n",
       " \"'\",\n",
       " 'm',\n",
       " '19',\n",
       " 'yrs',\n",
       " 'old',\n",
       " '.']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize\n",
    "wordpunct_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1590a625-1b72-4697-8002-fc8f41e929fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " ',',\n",
       " 'I',\n",
       " \"'m\",\n",
       " 'krrish',\n",
       " 'mahar',\n",
       " ',',\n",
       " 'I',\n",
       " \"'m\",\n",
       " '19',\n",
       " 'yrs',\n",
       " 'old',\n",
       " '.']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "TreebankWordTokenizer().tokenize(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e7a3a7-8c6b-46df-bc5d-a18958928dd9",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "08c9944e-9985-43a1-97cc-f671039051a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'code'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "stemmer.stem('coding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0a3a0d47-4724-430f-a578-c45dca6375e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bath'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import RegexpStemmer\n",
    "reg_Stemmer = RegexpStemmer('ing$|s$|e$|able$', min=4)\n",
    "\n",
    "reg_Stemmer.stem('bathing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "74ecc69e-c6be-4432-9646-daf3660d6865",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ingeat'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_Stemmer.stem('ingeating')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2a578a-3155-455e-91e8-9f85fded51ec",
   "metadata": {},
   "source": [
    "### SnowBall Stemmer > Porter Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "47e109b2-0c71-4f08-b31d-061a61a94946",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "snowball = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b8440c7f-99ac-4220-84b2-87e8f4c33dd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fairli supportingli\n",
      "fair support\n"
     ]
    }
   ],
   "source": [
    "print(stemmer.stem('fairly'), stemmer.stem('supportingly'))\n",
    "\n",
    "print(snowball.stem('fairly'), snowball.stem('supportingly'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3852c686-9a9f-48dd-9829-8a8b99d57f02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'goe'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# But Stemming still fails on some words, so we use Lemmetization\n",
    "snowball.stem('goes')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476feeea-5f14-4c1f-aec7-5fd88000699f",
   "metadata": {},
   "source": [
    "### Wordnet Lemmatizer\n",
    "#### takes more time than stemming\n",
    "#### Used in QNA, Chatbox, text summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ec7c38bc-4cd1-4da7-9e4e-ff5964c1b0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a23cf6c4-77ab-4ff5-a3c2-6cf90b5c4b2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'go'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# POS TAG\n",
    "'''POS- Noun-n\n",
    "verb-v\n",
    "adjective-a\n",
    "adverb-r'''\n",
    "lemmatizer.lemmatize(\"goes\", pos=)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d1a05f75-d294-4e44-9090-c427173e0057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "betrothed----->betroth\n",
      "walking----->walk\n",
      "running----->run\n",
      "gone----->go\n",
      "am,----->am,\n",
      "history,----->history,\n",
      "goes----->go\n",
      "studies----->study\n",
      "ate----->eat\n",
      "supporting----->support\n",
      "fairly----->fairly\n"
     ]
    }
   ],
   "source": [
    "words = [\"betrothed\",\"walking\",\"running\",\"gone\",\"am,\", \"history,\" ,\"goes\",\"studies\",\"ate\",\"supporting\", \"fairly\"]\n",
    "for word in words:\n",
    "    print(word+\"----->\"+lemmatizer.lemmatize(word, pos='v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3605011b-22d3-469b-b484-cba9e8adfd10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
