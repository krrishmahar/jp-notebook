





import pandas as pd
import numpy as np
import matplotlib.pyplot as plt





from sklearn import datasets
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.svm import LinearSVC


iris = datasets.load_iris()
X = iris["data"][:, (2, 3)] # petal length, petal width
y = (iris["target"] == 2).astype(np.float64) # Iris virginica


print(type(X), y.dtype)


# svm_clf.predict([[5.5, 1.7]])











# Non-linear SVM
from sklearn.datasets import make_moons
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import PolynomialFeatures

X, y = make_moons(n_samples=10000, noise=0.15)


from sklearn.calibration import CalibratedClassifierCV
base_polynomial_svm_clf = Pipeline([
("poly_features", PolynomialFeatures(degree=3)),
("scaler", StandardScaler()),
("svm_clf", LinearSVC(C=10, loss="hinge"))
])
poly_svm_clf = CalibratedClassifierCV(base_polynomial_svm_clf)


svm_clf = Pipeline([
("scaler", StandardScaler()),
("linear_svc", LinearSVC(C=1, loss="hinge")),
])


from sklearn.calibration import CalibratedClassifierCV
base_svm_clf = Pipeline([
("scaler", StandardScaler()),
("linear_svc", LinearSVC(C=1, loss="hinge"))
])

svm_clf=CalibratedClassifierCV(svm_clf)


# polynomial_svm_clf.fit(X, y)





from sklearn.svm import SVC

ploy_kernal_svm_clf = Pipeline([
    ("scalar", StandardScaler()),
    ("svm_clf",SVC(kernel='poly', degree=3, coef0=1, C=5))
])


ploy_kernal_svm_clf.fit(X,y)








rbf_kernel_svm_clf = Pipeline([
("scaler", StandardScaler()),
("svm_clf", SVC(kernel="rbf", gamma=5, C=0.001))
])
# rbf_kernel_svm_clf.fit(X, y)





from sklearn.svm import SVR
svm_poly_reg = SVR(kernel="poly", degree=2, C=100, epsilon=0.1)
# svm_poly_reg.fit(X, y)


import  os
import pandas as pd
import numpy as np
DATA_DIR= 'ml-data'
def create_file(data, FILE_NAME) -> str:
    if (type(data) == np.ndarray):
        df = pd.DataFrame(data)
        os.makedirs(DATA_DIR, exist_ok=True)
        file_path = os.path.join(DATA_DIR, FILE_NAME)
        df.to_csv(file_path, index=False)
        return f'File saved to ${DATA_DIR}'
    return "file: ${FILE_NAME} should be of numpy.ndarray type"


create_file(X,'training_data')
create_file(y,'test_data')








from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3)





from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import VotingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC

log_clf = LogisticRegression()
rnd_clf = RandomForestClassifier()
# svm_clf = SVC()
voting_clf = VotingClassifier(
    estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf), ('poly_svc', poly_svm_clf)],
    voting='hard')

voting_clf.fit(X_train, y_train)


from sklearn.metrics import accuracy_score
for clf in (log_clf, rnd_clf, svm_clf, voting_clf):
    clf.fit(X_train, y_train)
    y_pred = clf.predict(X_test)
    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))





from sklearn.ensemble import BaggingClassifier
from sklearn.tree import DecisionTreeClassifier

bag_clf = BaggingClassifier(
    DecisionTreeClassifier(), n_estimators=500,
    max_samples=100, bootstrap=True, n_jobs=-1)

bag_clf.fit(X_train, y_train)

y_pred_b = bag_clf.predict(X_test)


paste_clf = BaggingClassifier(
    DecisionTreeClassifier(), n_estimators=500,
    max_samples=100, bootstrap=False, n_jobs=-1)

paste_clf.fit(X_train, y_train)

y_pred_p = bag_clf.predict(X_test)


from sklearn.model_selection import cross_val_score
scores = cross_val_score(bag_clf, X_train, y_train, cv=5, scoring='accuracy' )
scores


from sklearn.model_selection import cross_val_predict
cross_val_predict(bag_clf, X_train, y_train, cv=5,)


y_train.shape, y_test.shape, y_pred.shape


from sklearn.metrics import confusion_matrix
confusion_matrix(y_test, y_pred_b)


from sklearn.metrics import confusion_matrix
confusion_matrix(y_test, y_pred_p)


from sklearn.metrics import accuracy_score
# print(bag_clf.predict_proba(X_test))
print(bag_clf.__class__.__name__, accuracy_score(y_test, y_pred))








from sklearn.ensemble import BaggingClassifier

bag_class = BaggingClassifier(
    DecisionTreeClassifier(), n_estimators=500, 
    bootstrap=True, n_jobs=-1,
    oob_score=True
)
bag_class.fit(X_train, y_train)
bag_class.oob_score_





from sklearn.model_selection import cross_val_score
scores = cross_val_score(bag_clf, X_train, y_train, cv=5, scoring='accuracy' )
scores


from sklearn.metrics import accuracy_score
y_pred = bag_clf.predict(X_test)
accuracy_score(y_test, y_pred)


from sklearn.metrics import confusion_matrix
confusion_matrix(y_test, y_pred)





from sklearn.ensemble import RandomForestClassifier
rnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, n_jobs=-1)
rnd_clf.fit(X_train, y_train)
y_pred_rf = rnd_clf.predict(X_test)





from sklearn.ensemble import ExtraTreesClassifier
ext_clf = ExtraTreesClassifier(n_estimators=500, max_leaf_nodes=15,n_jobs=-1)
ext_clf.fit(X_train, y_train)
y_pred_et = ext_clf.predict(X_test)


from sklearn.metrics import accuracy_score
print(rnd_clf.__class__.__name__, accuracy_score(y_test, y_pred_rf))


from sklearn.metrics import accuracy_score
print(ext_clf.__class__.__name__, accuracy_score(y_test, y_pred_et))








from sklearn.datasets import load_iris
iris = load_iris()
rnd_clf = RandomForestClassifier(n_estimators=500, n_jobs=-1)
rnd_clf.fit(iris["data"], iris["target"])
for name, score in zip(iris["feature_names"], rnd_clf.feature_importances_):
    print(name, score)











from sklearn.tree import DecisionTreeRegressor
tree_reg1 = DecisionTreeRegressor(max_depth=2)
tree_reg1.fit(X, y)


y2 = y - tree_reg1.predict(X)
tree_reg2 = DecisionTreeRegressor(max_depth=2)
tree_reg2.fit(X, y2)


y3 = y2 - tree_reg2.predict(X)
tree_reg3 = DecisionTreeRegressor(max_depth=2)
tree_reg3.fit(X, y3)


y_pred = sum(tree.predict(X) for tree in (tree_reg1, tree_reg2, tree_reg3))


y_pred


from sklearn.ensemble import GradientBoostingRegressor
gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=3, learning_rate=1.0)
gbrt.fit(X, y)





import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

X_train, X_test, y_train, y_test = train_test_split(X,y)


X_train.shape, X_test.shape,y_train.shape, y_test.shape


from sklearn.ensemble import GradientBoostingRegressor
gbrt =  GradientBoostingRegressor(max_depth=2, n_estimators=120)
gbrt.fit(X_train, y_train)





from sklearn.datasets import load_diabetes
X, y = load_diabetes(return_X_y=True, as_frame=True)


import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2)

X_train.shape, X_test.shape, y_train.shape, y_test.shape


from sklearn.ensemble import GradientBoostingRegressor
gbrt =  GradientBoostingRegressor(max_depth=2, n_estimators=120)
gbrt.fit(X_train, y_train)


errors = [mean_squared_error(y_test, y_pred)
         for y_pred in gbrt.staged_predict(X_test)]
bst_estimators = np.argmin(errors) + 1 

bst_estimators





gbrt = GradientBoostingRegressor(max_depth=2, warm_start=True)
min_val_error = float("inf")
error_going_up = 0
for n_estimators in range(1, 120):
    gbrt.n_estimators = n_estimators
    gbrt.fit(X_train, y_train)
    y_pred = gbrt.predict(X_test)
    val_error = mean_squared_error(y_test, y_pred)
    if val_error < min_val_error:
        min_val_error = val_error
        error_going_up = 0
    else:
        error_going_up += 1
        if error_going_up == 5:
            break # early stopping


n_estimators


gbrt_best = GradientBoostingRegressor(max_depth=2,n_estimators=n_estimators)
gbrt_best.fit(X_train, y_train)








from sklearn.ensemble import GradientBoostingRegressor
gbrt =  GradientBoostingRegressor(max_depth=2, n_estimators=120)
gbrt.fit(X_train, y_train)


y_pred_gbrt = gbrt.predict(X_test)
y_pred_gbrt_best = gbrt_best.predict(X_test)


# Residuals for both models
residuals_gbrt = y_test - y_pred_gbrt
residuals_best = y_test - y_pred_gbrt_best


# Plot histograms of residuals side-by-side
fig, axes = plt.subplots(1, 2, figsize=(10, 5))  # 1 row, 2 columns

# Histogram for y_pred_gbrt
axes[0].hist(residuals_gbrt, bins=20, color='steelblue')
axes[0].set_title('Histogram of Residuals (GBRT)')
axes[0].set_xlabel('Residuals')
axes[0].set_ylabel('Frequency')

# Histogram for y_pred_gbrt_best
axes[1].hist(residuals_best, bins=20, color='seagreen')
axes[1].set_title('Histogram of Residuals (GBRT Best)')
axes[1].set_xlabel('Residuals')
axes[1].set_ylabel('Frequency')

plt.tight_layout()
plt.show()



import numpy as np

# Scale-Location Plot for y_pred_gbrt
sqrt_abs_resid = np.sqrt(np.abs(residuals_gbrt))
sqrt_abs_resid_best = np.sqrt(np.abs(residuals_best))


# Plot histograms of residuals side-by-side
fig, axes = plt.subplots(1, 2, figsize=(12, 5))  # 1 row, 2 columns

axes[0].scatter(y_pred_gbrt, sqrt_abs_resid)
axes[0].set_title("Scale-Location Plot (GBRT)")
axes[0].set_xlabel("Predicted Values")
axes[0].set_ylabel("Sqrt(|Residuals|)")

axes[1].scatter(y_pred_gbrt_best, sqrt_abs_resid_best)
axes[1].set_title("Scale-Location Plot (GBRT- Best)")
axes[1].set_xlabel("Predicted Values")
axes[1].set_ylabel("Sqrt(|Residuals|)")




import matplotlib.pyplot as plt
import statsmodels.api as sm

# Create subplots with 1 row, 2 columns
fig, axes = plt.subplots(1, 2, figsize=(9, 4))

# QQ plot for residuals from GBRT model
sm.qqplot(residuals_gbrt, fit=True, line="45", alpha=0.2, ax=axes[0])
axes[0].set_title("QQ Plot - GBRT Residuals")

# QQ plot for residuals from GBRT Best model
sm.qqplot(residuals_best, fit=True, line="45", alpha=0.2, ax=axes[1])
axes[1].set_title("QQ Plot - GBRT Best Residuals")

plt.tight_layout()
plt.show()



fig, ax = plt.subplots(figsize=(6, 4))
ax.scatter(y_test, y_pred_gbrt)
ax.plot([y.min(), y.max()], [y.min(), y.max()], "k--", lw=2)
ax.set_xlabel("Actual")
ax.set_ylabel("Predicted")
ax.set_title("Actual vs. Predicted")
plt.show()








import xgboost

xgb_reg = xgboost.XGBRegressor()
xgb_reg.fit(X_train, y_train)
y_pred = xgb_reg.predict(X_test)





xgb_reg = xgboost.XGBRegressor( early_stopping_rounds=2)
xgb_reg.fit(X_train, y_train, eval_set=[(X_test, y_test)])

y_pred = xgb_reg.predict(X_test)



