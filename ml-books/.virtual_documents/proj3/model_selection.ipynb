























import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.datasets import load_iris 

iris = load_iris()
iris['target_names']


fig = plt.figure(1, figsize=(8, 6))
ax = fig.add_subplot(111, projection="3d", elev=-150, azim=110)

pca = PCA(n_components=3).fit_transform(iris.data)
ax.scatter(
    pca[:, 0],
    pca[:, 1],
    pca[:, 2],
    c=iris.target,
    s=40,
)

ax.set_title("First three PCA dimensions")
ax.set_xlabel("1st Eigenvector")
ax.xaxis.set_ticklabels([])
ax.set_ylabel("2nd Eigenvector")
ax.yaxis.set_ticklabels([])
ax.set_zlabel("3rd Eigenvector")
ax.zaxis.set_ticklabels([])

plt.show()


pca.explained_variance_ratio_


import matplotlib.pyplot as plt

_, ax = plt.subplots()
scatter = ax.scatter(iris.data[:, 0], iris.data[:, 1], c=iris.target)
ax.set(xlabel=iris.feature_names[0], ylabel=iris.feature_names[1])
_ = ax.legend(
    scatter.legend_elements()[0], iris.target_names, loc="lower right", title="Classes"
)


pca = PCA().fit(iris.data)
cumsum = np.cumsum(pca.explained_variance_ratio_)
d = np.argmax(cumsum >= 0.95) + 1
d


pca = PCA(n_components=0.95) # instead of no. of PC we set it to float between 0.0 and 1.0, indicating the ratio of variance 
X_reduced = pca.fit_transform(iris.data)








import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# Import datasets, classifiers and performance metrics
from sklearn import datasets, metrics, svm
from sklearn.model_selection import train_test_split



digits_train = pd.read_csv('train/fashion-mnist_train.csv')
digits_test = pd.read_csv('test/fashion-mnist_test.csv')


# Split into features and labels
X_train, y_train = digits_train.iloc[:, 1:], digits_train.iloc[:, 0]
X_test, y_test = digits_test.iloc[:, 1:], digits_test.iloc[:, 0]

# Optionally view shape or data
X_train.shape, y_train.shape


y_train


import time
from sklearn.svm import SVC

start = time.time()
# Use only 10,000 samples for training
sample_size = 11000
X_small = X_train_reduced[:sample_size]
y_small = y_train[:sample_size]
# X_small = X_train_reduced
# y_small = y_train

clf = SVC(gamma=0.001)
clf.fit(X_small, y_small)

end = time.time()
# Predict the value of the digit on the test subset
predicted = clf.predict(X_test_reduced)
total_time = end-start
print(
    f"Classification report for classifier {clf.__class__.__name__}:\n"
    f"{metrics.classification_report(y_test, predicted)}\n"
    f"Total time taken to train: {total_time}"
)


X_train_reduced.shape


from sklearn.decomposition import PCA
pca = PCA().fit(X_train)
cumsum = np.cumsum(pca.explained_variance_ratio_)
d = np.argmax(cumsum >= 0.98) + 1
d





from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

pca = PCA(n_components=200)
X_train_reduced = pca.fit_transform(X_train_scaled)
X_test_reduced = pca.transform(X_test_scaled)









from sklearn import metrics

print(
    f"Classification report for classifier {clf}:\n"
    f"{metrics.classification_report(y_test, predicted)}\n"
)


import matplotlib.pyplot as plt

pca = PCA().fit(X_train_scaled)
plt.plot(np.cumsum(pca.explained_variance_ratio_))
plt.xlabel('Number of Components')
plt.ylabel('Cumulative Explained Variance')
plt.grid(True)
plt.show()










