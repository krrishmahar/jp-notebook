

















import pandas as pd
import numpy as np
import seaborn as sns
import sklearn

df = pd.read_csv('customer_data.csv')

df.head()

df.reset_index(drop=True,inplace=True)

### Customer Segmentation, using all features

from sklearn.cluster import KMeans
kmeans = KMeans(n_clusters=5, random_state=42)
kmeans.fit(X)

labels = kmeans.labels_
labels

y_kmeans = kmeans.fit_predict(X)
y_kmeans is kmeans.labels_


# Center points are not distinct enough
kmeans.cluster_centers_


cols = ["Fresh", "Milk", "Grocery", "Frozen"]
sns.pairplot(df[cols])
plt.show()






from sklearn.preprocessing import StandardScaler
import numpy as np

# Log-transform to reduce skew (add 1 to avoid log(0))
X_log = np.log1p(X)

# Standardize
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_log)



import matplotlib.pyplot as plt
from sklearn.cluster import KMeans

# X = df.drop(columns=["Channel", "Region"]).values 
wcss = []
for i in range(1,11):
  kmeans = KMeans(n_clusters= i, init= 'k-means++', random_state= 42)
  kmeans.fit(X_scaled)
  wcss.append(kmeans.inertia_)
plt.plot(range(1,11) , wcss)
plt.title('The Elbow Method')
plt.xlabel('Numbers of Clusters')
plt.ylabel('WCSS')
plt.show





from sklearn.decomposition import PCA
import seaborn as sns
import matplotlib.pyplot as plt

pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)

sns.scatterplot(x=X_pca[:,0], y=X_pca[:,1], hue=kmeans.labels_, palette='Set2')
plt.title("Clusters visualized using PCA")
plt.show()






from sklearn.metrics import silhouette_score

kmeans = KMeans(n_clusters=5, random_state=42)
kmeans.fit(X_scaled)
score = silhouette_score(X_scaled, kmeans.labels_)
print("Silhouette Score:", score)






from sklearn.metrics import davies_bouldin_score
score = davies_bouldin_score(X_scaled, kmeans.labels_)
print("DB Index:", score)  # Lower is better






import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D  # Ensure 3D plotting is enabled
import numpy as np

fig = plt.figure(1, figsize=(8, 6))
ax = fig.add_subplot(111, projection="3d", elev=-150, azim=110)

pca = PCA(n_components=3).fit_transform(X_scaled)
labels = kmeans.labels_
unique_labels = np.unique(labels)
colors = plt.cm.Set2(np.linspace(0, 1, len(unique_labels)))

# Plot each cluster separately to get correct legend
for i, label in enumerate(unique_labels):
    ax.scatter(
        pca[labels == label, 0],
        pca[labels == label, 1],
        pca[labels == label, 2],
        s=40,
        color=colors[i],
        label=f"Cluster {label}"
    )

ax.set_title("First three PCA dimensions")
ax.set_xlabel("1st Eigenvector")
ax.set_ylabel("2nd Eigenvector")
ax.set_zlabel("3rd Eigenvector")

plt.legend()
plt.show()






df = pd.read_csv('Mall_Customers.csv')
df.head()


X_scaled = df.drop(columns=['Genre','CustomerID']).values


import matplotlib.pyplot as plt
from sklearn.cluster import KMeans

# X = df.drop(columns=["Channel", "Region"]).values 
wcss = []
for i in range(1,11):
  kmeans = KMeans(n_clusters= i, init= 'k-means++', random_state= 42)
  kmeans.fit(X_aniso)
  wcss.append(kmeans.inertia_)
plt.plot(range(1,11) , wcss)
plt.title('The Elbow Method')
plt.xlabel('Numbers of Clusters')
plt.ylabel('WCSS')
plt.show


from sklearn.cluster import KMeans
kmeans = KMeans(n_clusters=3)
kmeans.fit(X_aniso)





import sklearn.datasets as datasets
n_samples=500
random_state = 170
X, y = datasets.make_blobs(n_samples=n_samples, random_state=random_state)
transformation = [[0.6, -0.6], [-0.4, 0.8]]
X_aniso = np.dot(X, transformation)
aniso = (X_aniso, y)






from sklearn.decomposition import PCA
import seaborn as sns
import matplotlib.pyplot as plt

pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_aniso)

sns.scatterplot(x=X_pca[:,0], y=X_pca[:,1], hue=kmeans.labels_, palette='Set2')
plt.title("Clusters visualized using PCA")
plt.show()



from sklearn.decomposition import PCA
import seaborn as sns
import matplotlib.pyplot as plt

pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)

sns.scatterplot(x=X_pca[:,0], y=X_pca[:,1], hue=kmeans.labels_, palette='Set2')
plt.title("Clusters visualized using PCA")
plt.show()






from sklearn.metrics import silhouette_score

kmeans = KMeans(n_clusters=5, random_state=42)
kmeans.fit(X_scaled)
score = silhouette_score(X_scaled, kmeans.labels_)
print("Silhouette Score:", score)



from sklearn.metrics import silhouette_score

kmeans = KMeans(n_clusters=3, random_state=42)
kmeans.fit(X_aniso)
score = silhouette_score(X_aniso, kmeans.labels_)
print("Silhouette Score:", score)






from sklearn.metrics import davies_bouldin_score
score = davies_bouldin_score(X_scaled, kmeans.labels_)
print("DB Index:", score)  # Lower is better



from sklearn.metrics import davies_bouldin_score
score = davies_bouldin_score(X_aniso, kmeans.labels_)
print("DB Index:", score)  # Lower is better






np.unique(kmeans.labels_)


import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D  # Ensure 3D plotting is enabled
import numpy as np

fig = plt.figure(1, figsize=(8, 6))
ax = fig.add_subplot(111, projection="3d", elev=-150, azim=110)

pca = PCA().fit_transform(X_aniso)
labels = kmeans.labels_
unique_labels = np.unique(labels)
colors = plt.cm.Set2(np.linspace(0, 1, len(unique_labels)))

# Plot each cluster separately to get correct legend
for i, label in enumerate(unique_labels):
    ax.scatter(
        pca[labels == label, 0],
        pca[labels == label, 1],
        # pca[labels == label, 2],
        s=40,
        color=colors[i],
        label=f"Cluster {label}"
    )

ax.set_title("First three PCA dimensions")
ax.set_xlabel("1st Eigenvector")
ax.set_ylabel("2nd Eigenvector")
ax.set_zlabel("3rd Eigenvector")

plt.legend()
plt.show()




