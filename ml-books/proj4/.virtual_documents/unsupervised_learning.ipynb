




















import pandas as pd
import numpy as np
import seaborn as sns
import sklearn
import matplotlib.pyplot as plt
%matplotlib inline



df = pd.read_csv('customer_data.csv')

df.head()

df.reset_index(drop=True,inplace=True)

### Customer Segmentation, using all features

from sklearn.cluster import KMeans
kmeans = KMeans(n_clusters=5, random_state=42)
kmeans.fit(X)

labels = kmeans.labels_
labels

y_kmeans = kmeans.fit_predict(X)
y_kmeans is kmeans.labels_


# Center points are not distinct enough
kmeans.cluster_centers_


cols = ["Fresh", "Milk", "Grocery", "Frozen"]
sns.pairplot(df[cols])
plt.show()






from sklearn.preprocessing import StandardScaler
import numpy as np

# Log-transform to reduce skew (add 1 to avoid log(0))
X_log = np.log1p(X)

# Standardize
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_log)



import matplotlib.pyplot as plt
from sklearn.cluster import KMeans

# X = df.drop(columns=["Channel", "Region"]).values 
wcss = []
for i in range(1,11):
  kmeans = KMeans(n_clusters= i, init= 'k-means++', random_state= 42)
  kmeans.fit(X_scaled)
  wcss.append(kmeans.inertia_)
plt.plot(range(1,11) , wcss)
plt.title('The Elbow Method')
plt.xlabel('Numbers of Clusters')
plt.ylabel('WCSS')
plt.show





from sklearn.decomposition import PCA
import seaborn as sns
import matplotlib.pyplot as plt

pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)

sns.scatterplot(x=X_pca[:,0], y=X_pca[:,1], hue=kmeans.labels_, palette='Set2')
plt.title("Clusters visualized using PCA")
plt.show()






from sklearn.metrics import silhouette_score

kmeans = KMeans(n_clusters=5, random_state=42)
kmeans.fit(X_scaled)
score = silhouette_score(X_scaled, kmeans.labels_)
print("Silhouette Score:", score)






from sklearn.metrics import davies_bouldin_score
score = davies_bouldin_score(X_scaled, kmeans.labels_)
print("DB Index:", score)  # Lower is better






import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D  # Ensure 3D plotting is enabled
import numpy as np

fig = plt.figure(1, figsize=(8, 6))
ax = fig.add_subplot(111, projection="3d", elev=-150, azim=110)

pca = PCA(n_components=3).fit_transform(X_scaled)
labels = kmeans.labels_
unique_labels = np.unique(labels)
colors = plt.cm.Set2(np.linspace(0, 1, len(unique_labels)))

# Plot each cluster separately to get correct legend
for i, label in enumerate(unique_labels):
    ax.scatter(
        pca[labels == label, 0],
        pca[labels == label, 1],
        pca[labels == label, 2],
        s=40,
        color=colors[i],
        label=f"Cluster {label}"
    )

ax.set_title("First three PCA dimensions")
ax.set_xlabel("1st Eigenvector")
ax.set_ylabel("2nd Eigenvector")
ax.set_zlabel("3rd Eigenvector")

plt.legend()
plt.show()






df = pd.read_csv('Mall_Customers.csv')
df.head()


X_scaled = df.drop(columns=['Genre','CustomerID']).values


from sklearn.datasets import make_blobs

centers=[(0,5),(4,3),(8,2),(4,8),(9,9)]
X, y = make_blobs(n_samples=500,centers=centers, cluster_std=0.65, random_state=0,)
# print(centers)
plt.scatter(X[:, 0], X[:, 1], c=y)
plt.title("Three normally-distributed clusters")
plt.show()


from scipy.spatial import Voronoi, voronoi_plot_2d

# Fit KMeans
kmeans = KMeans(n_clusters=5, random_state=0)
kmeans.fit(X)
centroids = kmeans.cluster_centers_

# Create Voronoi diagram from centroids
vor = Voronoi(centroids)

# Plot
fig, ax = plt.subplots(figsize=(8, 6))

# Plot Voronoi diagram first
voronoi_plot_2d(vor, ax=ax, show_vertices=False, line_colors='k', line_width=2, line_alpha=0.6, point_size=0)

# Fill regions after voronoi_plot_2d (to keep them on top)
colors =['#FF0000', '#00FF00', '#0000FF','#FFFF00','#800080']
# colorize
for i, region in enumerate(regions):
    polygon = vertices[region]
    plt.fill(*zip(*polygon), color=colors[i % len(colors)], alpha=0.4)

plt.plot(points[:,0], points[:,1], 'ko')
plt.xlim(vor.min_bound[0] - 0.1, vor.max_bound[0] + 0.1)
plt.ylim(vor.min_bound[1] - 0.1, vor.max_bound[1] + 0.1)

# Plot data points
ax.scatter(X[:, 0], X[:, 1], c=kmeans.labels_, cmap='prism', s=30, alpha=0.8)
ax.scatter(centroids[:, 0], centroids[:, 1], c='black', s=100, marker='X')
ax.set_title("KMeans Clustering with Voronoi Diagram and Background Coloring")
plt.tight_layout()
plt.show()


from scipy.spatial import Voronoi, voronoi_plot_2d

# Fit KMeans
kmeans = KMeans(n_clusters=5, random_state=0)
kmeans.fit(X)
centroids = kmeans.cluster_centers_

# Create Voronoi diagram from centroids
vor = Voronoi(centroids)

# Plot
fig, ax = plt.subplots(figsize=(8, 6))

# Plot Voronoi diagram first
voronoi_plot_2d(vor, ax=ax, show_vertices=False, line_colors='k', line_width=2, line_alpha=0.6, point_size=0)

# Fill regions after voronoi_plot_2d (to keep them on top)
colors =['#FF0000', '#00FF00', '#0000FF','#FFFF00','#800080']
# colorize

# colorize
for i, region in enumerate(regions):
    polygon = vertices[region]
    plt.fill(*zip(*polygon), color=colors[i % len(colors)], alpha=0.4)

plt.plot(points[:,0], points[:,1], 'ko')
plt.xlim(vor.min_bound[0] - 0.1, vor.max_bound[0] + 0.1)
plt.ylim(vor.min_bound[1] - 0.1, vor.max_bound[1] + 0.1)

# Plot data points
ax.scatter(X[:, 0], X[:, 1], c=kmeans.labels_, cmap='prism', s=30, alpha=0.8)
ax.scatter(centroids[:, 0], centroids[:, 1], c='black', s=100, marker='X')
ax.set_title("KMeans Clustering with Voronoi Diagram and Background Coloring")
plt.tight_layout()
plt.show()


import matplotlib.pyplot as plt
from sklearn.cluster import KMeans

# X = df.drop(columns=["Channel", "Region"]).values 
wcss = []
for i in range(1,11):
  kmeans = KMeans(n_clusters= i, init= 'k-means++', random_state= 42)
  kmeans.fit(X)
  wcss.append(kmeans.inertia_)
plt.plot(range(1,11) , wcss)
plt.title('The Elbow Method')
plt.xlabel('Numbers of Clusters')
plt.ylabel('WCSS')
plt.show()


from sklearn.cluster import KMeans
kmeans = KMeans(n_clusters=5)
kmeans.fit(X)


import sklearn.datasets as datasets
n_samples=500
random_state = 170
X, y = datasets.make_blobs(n_samples=n_samples, random_state=random_state)
transformation = [[0.6, -0.6], [-0.4, 0.8]]
X_aniso = np.dot(X, transformation)
aniso = (X_aniso, y)



from sklearn.datasets import make_blobs

centers=[(0,5),(4,3),(8,2),(4,8),(9,9)]
X, y = make_blobs(n_samples=500,centers=centers, cluster_std=0.65, random_state=0,)

# Fit KMeans
good_init = np.array([[0,5],[4,3], [8,1] ,[4,8], [9,8]])
kmeans = KMeans(n_clusters=5, init='k-means++', n_init=11)
kmeans.fit(X)
centroids = kmeans.cluster_centers_



kmeans.inertia_, kmeans.score(X)


from scipy.spatial import Voronoi, voronoi_plot_2d
import matplotlib.pyplot as plt

# Create Voronoi diagram from centroids
vor = Voronoi(centroids)

# Plot
fig, ax = plt.subplots(figsize=(8, 6))

# Plot Voronoi edges
voronoi_plot_2d(vor, ax=ax, show_vertices=True, line_colors='k', line_width=2, line_alpha=0.6, point_size=0)

# Define colors
colors = ['#FF0000', '#00FF00', '#0000FF', '#FFFF00', '#800080']

# Fill valid Voronoi regions with colors
for i, region_idx in enumerate(vor.point_region):
    ax.fill(*zip(*polygon), color=colors[i % len(colors)], alpha=0.4)
    region = vor.regions[region_idx]
    polygon = [vor.vertices[j] for j in region]

# Optional: plot centroids again
ax.scatter(X[:, 0], X[:, 1], c=kmeans.labels_, cmap='prism', s=30, alpha=0.8)
ax.scatter(centroids[:, 0], centroids[:, 1], c='black', s=100, marker='X')

# Limits and title
ax.set_xlim(vor.min_bound[0] - 0.1, vor.max_bound[0] + 0.1)
ax.set_ylim(vor.min_bound[1] - 0.1, vor.max_bound[1] + 0.1)
ax.set_title("KMeans Clustering with Voronoi Diagram and Background Coloring")

plt.tight_layout()
plt.show()


from sklearn.decomposition import PCA
import seaborn as sns
import matplotlib.pyplot as plt

pca = PCA()
X_pca = pca.fit_transform(X)

len(X_pca[:,0]), len(X_pca[:,1]),y.shape, len(kmeans.labels_)


from sklearn.decomposition import PCA
import seaborn as sns
import matplotlib.pyplot as plt

pca = PCA()
X_pca = pca.fit_transform(X)

sns.scatterplot(x=X_pca[:,0], y=X_pca[:,1], hue=kmeans.labels_, palette='Set2')
plt.title("Clusters visualized using PCA")
plt.show()






from sklearn.metrics import silhouette_score

kmeans = KMeans(n_clusters=5, random_state=42)
kmeans.fit(X)
score = silhouette_score(X, kmeans.labels_)
print("Silhouette Score:", score)






from sklearn.metrics import davies_bouldin_score
score = davies_bouldin_score(X, kmeans.labels_)
print("DB Index:", score)  # Lower is better






import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D  # Ensure 3D plotting is enabled
import numpy as np

fig = plt.figure(1, figsize=(8, 6))
ax = fig.add_subplot(111, projection="3d", elev=-150, azim=110)

pca = PCA().fit_transform(X)
labels = kmeans.labels_
unique_labels = np.unique(labels)
print(unique_labels) # sorry,here labels are unique values like 0,1,2,3,4

colors = plt.cm.Set2(np.linspace(0, 1, len(unique_labels)))

# Plot each cluster separately to get correct legend
for i, label in enumerate(unique_labels):
    ax.scatter(
        # here labels are used but as X axis, since we have used  2 features (X axis,Y axis)
        pca[labels == label, 0],
        pca[labels == label, 1],
        s=40,
        # color=colors[i],
        label=f"Cluster {label}"
    )

ax.set_title("First three PCA dimensions")
ax.set_xlabel("1st Eigenvector")
ax.set_ylabel("2nd Eigenvector")
ax.set_zlabel("3rd Eigenvector")

plt.legend()
plt.show()






import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D  # Ensure 3D plotting is enabled
import numpy as np

X, _ = make_blobs(n_samples=5000, cluster_std=0.65, random_state=0)

kmeans = KMeans(n_clusters= i, init= 'k-means++', random_state= 42).fit(X)
print(kmeans.labels_)

fig = plt.figure(1, figsize=(8, 6))
ax = fig.add_subplot(111, projection="3d", elev=-150, azim=110)

pca = PCA().fit_transform(X)
labels = kmeans.labels_
unique_labels = np.unique(labels)
colors = plt.cm.Set2(np.linspace(0, 1, len(unique_labels)))

# Plot each cluster separately to get correct legend
for i, label in enumerate(unique_labels):
    ax.scatter(
        pca[labels == label, 0],
        pca[labels == label, 1],
        pca[labels == label, 2],
        s=40,
        color=colors[i],
        label=f"Cluster {label}"
    )

ax.set_title("First three PCA dimensions")
ax.set_xlabel("1st Eigenvector")
ax.set_ylabel("2nd Eigenvector")
ax.set_zlabel("3rd Eigenvector")

plt.legend()
plt.show()



from sklearn.cluster import MiniBatchKMeans
# Generate sample clustered data
X, _ = make_blobs(n_samples=5000, cluster_std=0.65, random_state=0)

# Range of cluster numbers to try
k_values = range(2, 9)
silhouette_scores = []

# Compute silhouette scores for each k
for k in k_values:
    mb_kmeans = MiniBatchKMeans(n_clusters=k, random_state=0, batch_size=500)
    labels = mb_kmeans.fit_predict(X)
    score = silhouette_score(X, labels)
    silhouette_scores.append(score)

# Plot
plt.figure(figsize=(8, 5))
plt.plot(k_values, silhouette_scores, 'bo-')
plt.xlabel("k")
plt.ylabel("Silhouette score")
plt.title("Silhouette Score vs. Number of Clusters (MiniBatchKMeans)")
plt.grid(True)
plt.xticks(k_values)
plt.show()












from sklearn.metrics import silhouette_samples, silhouette_score
from sklearn.cluster import MiniBatchKMeans

# Generate synthetic data
X, _ = make_blobs(n_samples=5000, cluster_std=0.65, random_state=0)

# Range of cluster values
k_values = [3, 4, 5, 6]

# Set up 2x2 plot grid
fig, axs = plt.subplots(2, 2, figsize=(12, 10))
axs = axs.flatten()

for idx, k in enumerate(k_values):
    # Create and fit MiniBatchKMeans
    kmeans = MiniBatchKMeans(n_clusters=k, random_state=0, batch_size=500)
    cluster_labels = kmeans.fit_predict(X)

    # Compute silhouette score and individual coefficients
    silhouette_avg = silhouette_score(X, cluster_labels)
    sample_silhouette_values = silhouette_samples(X, cluster_labels)

    ax = axs[idx]
    y_lower = 10

    for i in range(k):
        # Aggregate silhouette scores for cluster i
        ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]
        ith_cluster_silhouette_values.sort()

        size_cluster_i = ith_cluster_silhouette_values.shape[0]
        y_upper = y_lower + size_cluster_i

        color = plt.cm.nipy_spectral(float(i) / k)
        ax.fill_betweenx(np.arange(y_lower, y_upper),
                         0, ith_cluster_silhouette_values,
                         facecolor=color, edgecolor=color, alpha=0.7)

        ax.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))
        y_lower = y_upper + 10  # Add space between clusters

    ax.set_title(f"k = {k}")
    ax.axvline(x=silhouette_avg, color="red", linestyle="--")
    ax.set_xlim([-0.1, 1])
    ax.set_ylim([0, len(X) + (k + 1) * 10])
    ax.set_xlabel("Silhouette Coefficient")
    ax.set_ylabel("Cluster")

plt.suptitle("Silhouette Diagrams for Various k (MiniBatchKMeans)", fontsize=14)
plt.tight_layout(rect=[0, 0, 1, 0.96])
plt.show()










from matplotlib.image import imread
image = imread("butterfly.jpg")
image.shape


X = image.reshape(-1, 3)
kmeans = KMeans(n_clusters=8).fit(X)
segmented_img = kmeans.cluster_centers_[kmeans.labels_]
segmented_img = segmented_img.reshape(image.shape)


if segmented_img.max() > 1.0:
    segmented_img = segmented_img / 255

plt.axis('off')
plt.imshow(segmented_img)
plt.show()





from sklearn.datasets import load_digits
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression

X_digit, y_digit = load_digits(return_X_y=True)
X_train, X_test, y_train, y_test = train_test_split(X_digit, y_digit)

log_reg = LogisticRegression(max_iter=250)
log_reg.fit(X_train, y_train)


log_reg.score(X_test, y_test)


from sklearn.pipeline import Pipeline
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler

pipeline = Pipeline([
("kmeans", KMeans(n_init='auto', n_clusters=100)),
("standardize", StandardScaler()),
("log_reg", LogisticRegression()),
])
pipeline.fit(X_train, y_train)


pipeline.score(X_test, y_test)





from sklearn.model_selection import GridSearchCV


param_grid = dict(
    kmeans__n_clusters=range(20,100)
)
grid_clf = GridSearchCV(pipeline, param_grid, cv=5)
grid_clf.fit(X_train, y_train)
param_grid


grid_clf.best_params_, grid_clf.best_score_





n_labeled = 50
log_reg = LogisticRegression()
log_reg.fit(X_train[:n_labeled], y_train[:n_labeled])


log_reg.score(X_test, y_test)


k = 50
kmeans = KMeans(n_clusters=k)
X_digits_dist = kmeans.fit_transform(X_train)
representative_digit_idx = np.argmin(X_digits_dist, axis=0)
X_representative_digits = X_train[representative_digit_idx]


import matplotlib.pyplot as plt

plt.figure(figsize=(10, 5))
for idx, image in enumerate(X_representative_digits):
    plt.subplot(5, 10, idx + 1)
    plt.imshow(image.reshape(8, 8), cmap='binary')
    plt.title(str(y_representative_digits[idx]), fontsize=10)
    plt.axis('off')

plt.suptitle("50 Representative Digit Images (One per Cluster)", fontsize=14)
plt.tight_layout()
plt.show()


y_representative_digits = y_train[representative_digit_idx]
np.unique(y_representative_digits)


log_reg = LogisticRegression(n_jobs=-1, max_iter=500)
log_reg.fit(X_representative_digits, y_representative_digits)
log_reg.score(X_test, y_test)





y_train_propagated = np.empty(len(X_train), dtype=np.int32)
for i in range(k):
    y_train_propagated[kmeans.labels_==i] = y_representative_digits[i]


log_reg = LogisticRegression(max_iter=500,n_jobs=-1)
log_reg.fit(X_train, y_train_propagated)
log_reg.score(X_test, y_test)


percentile_closest = 20
X_cluster_dist = X_digits_dist[np.arange(len(X_train)), kmeans.labels_]
for i in range(k):
    in_cluster = (kmeans.labels_ == i)
    cluster_dist = X_cluster_dist[in_cluster]
    cutoff_distance = np.percentile(cluster_dist, percentile_closest)
    above_cutoff = (X_cluster_dist > cutoff_distance)
    X_cluster_dist[in_cluster & above_cutoff] = -1

partially_propagated = (X_cluster_dist != -1)
X_train_partially_propagated = X_train[partially_propagated]
y_train_partially_propagated = y_train_propagated[partially_propagated]


log_reg = LogisticRegression(max_iter=500,n_jobs=-1)
log_reg.fit(X_train_partially_propagated, y_train_partially_propagated)
log_reg.score(X_test, y_test)


np.mean(y_train_partially_propagated == y_train[partially_propagated])
