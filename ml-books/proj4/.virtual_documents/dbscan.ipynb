





import pandas as pd
import numpy as np
import seaborn as sns
import sklearn
import matplotlib.pyplot as plt
%matplotlib inline


from sklearn.cluster import DBSCAN
from sklearn.datasets import make_moons
X, y = make_moons(n_samples=1000, noise=0.05)
dbscan = DBSCAN(eps=0.05, min_samples=5)
dbscan.fit(X)


len(dbscan.core_sample_indices_)





from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier(n_neighbors=50)
knn.fit(dbscan.components_, dbscan.labels_[dbscan.core_sample_indices_])


X_new = np.array([[-0.5, 0], [0, 0.5], [1, -0.1], [2, 1]])
knn.predict(X_new)


knn.predict_proba(X_new)


y_dist, y_pred_idx = knn.kneighbors(X_new, n_neighbors=1)
y_pred = dbscan.labels_[dbscan.core_sample_indices_][y_pred_idx]
y_pred[y_dist > 0.2] = -1
y_pred.ravel()


X, y = make_moons(n_samples=1000, noise=0.05)

# Step 2: Apply DBSCAN clustering
dbscan = DBSCAN(eps=0.05, min_samples=5)
dbscan.fit(X)

# Step 3: Train k-NN on DBSCAN core points
core_samples = dbscan.components_  # shape (n_core_samples, 2)
core_labels = dbscan.labels_[dbscan.core_sample_indices_]
knn = KNeighborsClassifier(n_neighbors=50)
knn.fit(core_samples, core_labels)
# Step 4: Create mesh grid to visualize decision boundary
x1s = np.linspace(-1.5, 2.5, 100)
x2s = np.linspace(-1, 1.5, 100)
x1, x2 = np.meshgrid(x1s, x2s)
X_grid = np.c_[x1.ravel(), x2.ravel()]
y_knn_pred = knn.predict(X_grid)
y_knn_pred = y_knn_pred.reshape(x1.shape)

# Step 5: Predict a few new samples (optional)
X_new = np.array([[-0.5, 0], [0, 0.5], [1, -0.1], [2, 1]])
y_dist, y_pred_idx = knn.kneighbors(X_new, n_neighbors=1)
y_pred = core_labels[y_pred_idx.ravel()]
y_pred[y_dist.ravel() > 0.2] = -1  # Mark as outliers if too far

# Step 6: Plot
plt.figure(figsize=(8, 4))

# Decision boundary
plt.contourf(x1, x2, y_knn_pred, cmap='Paired', alpha=0.4)

# DBSCAN core points
plt.plot(X[:, 0], X[:, 1], 'k.', markersize=2)

# Query points
plt.plot(X_new[:, 0], X_new[:, 1], 'b+', markersize=15)

# Labels and decorations
plt.xlabel("$x_1$")
plt.ylabel("$x_2$")
plt.title("DBSCAN with k-NN Extrapolation of Decision Boundary")
plt.tight_layout()
plt.show()
























from sklearn.mixture import GaussianMixture
from sklearn.datasets import make_blobs

X1, y1 = make_blobs(n_samples=1000, centers=((4, -4), (0, 0)), random_state=42)
X1 = X1.dot(np.array([[0.374, 0.95], [0.732, 0.598]]))
X2, y2 = make_blobs(n_samples=250, centers=1, random_state=42)
X2 = X2 + [6, -8]
X = np.r_[X1, X2]
y = np.r_[y1, y2]

gm = GaussianMixture(n_components=5, n_init=20, covariance_type='spherical')
gm.fit(X)


plt.scatter(X[:, 0], X[:, 1], c=y)
plt.title("Three normally-distributed clusters")
plt.show()


gm.weights_


gm.means_


gm.covariances_


gm.converged_


gm.n_iter_


gm.predict(X)


gm.predict_proba(X)


X_new, y_new = gm.sample(6)
X_new


y_new


gm.score_samples(X_new)



def plot_centroids(centroids, weights=None, circle_color='w', cross_color='k'):
    if weights is not None:
        centroids = centroids[weights > weights.max() / 10]
    plt.scatter(centroids[:, 0], centroids[:, 1],
                marker='o', s=35, linewidths=8,
                color=circle_color, zorder=10, alpha=0.9)
    plt.scatter(centroids[:, 0], centroids[:, 1],
                marker='x', s=2, linewidths=12,
                color=cross_color, zorder=11, alpha=1)

def plot_decision_boundaries(clusterer, X, resolution=1000, show_centroids=True,
                             show_xlabels=True, show_ylabels=True):
    mins = X.min(axis=0) - 0.1
    maxs = X.max(axis=0) + 0.1
    xx, yy = np.meshgrid(np.linspace(mins[0], maxs[0], resolution),
                         np.linspace(mins[1], maxs[1], resolution))
    Z = clusterer.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)

    plt.contourf(Z, extent=(mins[0], maxs[0], mins[1], maxs[1]),
                cmap="Pastel2")
    plt.contour(Z, extent=(mins[0], maxs[0], mins[1], maxs[1]),
                linewidths=1, colors='k')
    plot_data(X)
    if show_centroids:
        plot_centroids(clusterer.cluster_centers_)

    if show_xlabels:
        plt.xlabel("$x_1$")
    else:
        plt.tick_params(labelbottom=False)
    if show_ylabels:
        plt.ylabel("$x_2$", rotation=0)
    else:
        plt.tick_params(labelleft=False)



from matplotlib.colors import LogNorm
from matplotlib.pyplot import savefig

def plot_gaussian_mixture(clusterer, X, resolution=1000, show_ylabels=True):
    mins = X.min(axis=0) - 0.1
    maxs = X.max(axis=0) + 0.1
    xx, yy = np.meshgrid(np.linspace(mins[0], maxs[0], resolution),
                         np.linspace(mins[1], maxs[1], resolution))
    Z = -clusterer.score_samples(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)

    plt.contourf(xx, yy, Z,
                 norm=LogNorm(vmin=1.0, vmax=30.0),
                 levels=np.logspace(0, 2, 12))
    plt.contour(xx, yy, Z,
                norm=LogNorm(vmin=1.0, vmax=30.0),
                levels=np.logspace(0, 2, 12),
                linewidths=1, colors='k')

    Z = clusterer.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)
    plt.contour(xx, yy, Z,
                linewidths=2, colors='r', linestyles='dashed')
    
    plt.plot(X[:, 0], X[:, 1], 'k.', markersize=2)
    plot_centroids(clusterer.means_, clusterer.weights_)

    plt.xlabel("$x_1$")
    if show_ylabels:
        plt.ylabel("$x_2$", rotation=0)
    else:
        plt.tick_params(labelleft=False)

plt.figure(figsize=(8, 4))

plot_gaussian_mixture(gm, X)

savefig("gaussian_mixtures_plot")
plt.show()






densities = gm.score_samples(X)
density_threshold = np.percentile(densities, 4)
anomalies = X[densities < density_threshold]












gm.bic(X)


gm.aic(X)


# extra code – this cell generates and saves Figure 9–20

gms_per_k = [GaussianMixture(n_components=k, n_init=10, random_state=42).fit(X)
             for k in range(1, 11)]
bics = [model.bic(X) for model in gms_per_k]
aics = [model.aic(X) for model in gms_per_k]

plt.figure(figsize=(8, 3))
plt.plot(range(1, 11), bics, "bo-", label="BIC")
plt.plot(range(1, 11), aics, "go--", label="AIC")
plt.xlabel("$k$")
plt.ylabel("Information Criterion")
plt.axis([1, 9.5, min(aics) - 50, max(aics) + 50])
plt.annotate("", xy=(3, bics[2]), xytext=(3.4, 8650),
             arrowprops=dict(facecolor='black', shrink=0.1))
plt.text(3.5, 8660, "Minimum", horizontalalignment="center")
plt.legend()
plt.grid()
savefig("aic_bic_vs_k_plot")
plt.show()


bics[2], aics[2]








from sklearn.mixture import BayesianGaussianMixture
bgm = BayesianGaussianMixture(n_components=10, n_init=10, max_iter=500)
bgm.fit(X)
np.round(bgm.weights_, 2)





gm = GaussianMixture(n_components=6, n_init=10, random_state=42).fit(X)
gm_best = GaussianMixture(n_components=3, n_init=10, random_state=42).fit(X)

print('Not Optimal: ',np.round(gm.weights_,2))
print('Optimal:     ',np.round(gm_best.weights_,2))












