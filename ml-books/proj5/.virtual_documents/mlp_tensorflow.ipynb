





import numpy as np
from sklearn.datasets import load_iris
from sklearn.linear_model import Perceptron

iris = load_iris()
X = iris.data[:, (2, 3)] # petal length, petal width
y = (iris.target == 0).astype(int) # Iris setosa?

per_clf = Perceptron()
per_clf.fit(X, y)

y_pred = per_clf.predict([[2, 0.5]])


y_pred ## yes, Iris Setosa
































import tensorflow as tf
from tensorflow import keras

tf.__version__


keras.__version__


fashion_mnist = keras.datasets.fashion_mnist
(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()


X_train.shape, X_train.dtype


X_valid, X_train = X_train[:5000] / 255.0, X_train[5000:] / 255.0
y_valid, y_train = y_train[:5000] , y_train[5000:]


class_names = ["T-shirt/top", "Trouser", "Pullover", "Dress", "Coat",
"Sandal", "Shirt", "Sneaker", "Bag", "Ankle boot"]


class_names[y_valid[0]]





# model = tf.keras.Sequential()
# model.add(keras.layers.Flatten(input_shape=[28,28]))
# model.add(keras.layers.Dense(300, activation='relu'))
# model.add(keras.layers.Dense(100, activation='relu'))
# model.add(keras.layers.Dense(10, activation='softmax'))



model = keras.models.Sequential([
    keras.layers.InputLayer(shape=(28,28)),
    keras.layers.Flatten(),
    keras.layers.Dense(300, activation="relu"),
    keras.layers.Dense(100, activation="relu"),
    keras.layers.Dense(10, activation="softmax")
])


model.summary()


# the first hidden layer has 784 Ã— 300 connection weights, plus 300 bias terms, which adds up to
# 235,500 parameters!
784*300+300


model.layers


hidden1 = model.layers[1]


weights, bias = hidden1.get_weights()
weights


weights.shape, bias.shape


model.compile(
    optimizer=keras.optimizers.SGD(learning_rate=0.1),
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy'])








history = model.fit(X_train, y_train, epochs=50, verbose=1, validation_data=(X_valid, y_valid), 
                    callbacks=[tensorboard_cb, early_stopping_cb])








history.params


# history.history  
# returns loss and extra metrics it measured at the end of each epoch of both training and validation set


import pandas as pd
import matplotlib.pyplot as plt
pd.DataFrame(history.history).plot(figsize=(8, 5))
plt.grid(True)
plt.gca().set_ylim(0, 1) # set the vertical range to [0-1]
plt.show()


model.evaluate(X_test, y_test)


!tree my_logs/


X_new = X_test[:3]
y_proba = model.predict(X_new)
y_proba.round(2)


predictions = model.predict(X_test)
predicted_classes = np.argmax(predictions, axis=-1)
predicted_classes


y_new = y_test[:3]
[class_names[y_val] for y_val in y_new]





from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

housing = fetch_california_housing()

X_train_full, X_test, y_train_full, y_test = train_test_split(
    housing.data, housing.target)

X_train, X_valid, y_train, y_valid = train_test_split(
    X_train_full, y_train_full, test_size=0.3)



import pandas as pd
df = pd.DataFrame(X_train)
df


# scaling the data
scaler = StandardScaler()

X_train = scaler.fit_transform(X_train)
X_valid = scaler.transform(X_valid)
X_test = scaler.transform(X_test)


X_train.shape[1:]


model = keras.Sequential([
    keras.layers.InputLayer(shape=(X_train.shape[1:])),
    keras.layers.Dense(30, activation='relu'),
    keras.layers.Dense(1)
])


model.compile(loss='mse', optimizer=keras.optimizers.SGD(learning_rate=1e-4))


history = model.fit(X_train, y_train, epochs=20, validation_data=(X_valid, y_valid), verbose=0)


mse_test = model.evaluate(X_test, y_test)
X_new = X_test[:3]
print(mse_test)
y_pred = model.predict(X_new)


y_pred, y_test[:3]











input_ = keras.layers.Input(shape=X_train.shape[1:])
hidden1 = keras.layers.Dense(30, activation="relu")(input_)
hidden2 = keras.layers.Dense(30, activation="relu")(hidden1)
concat = keras.layers.Concatenate()([input_, hidden2])
output = keras.layers.Dense(1)(concat)


model_1 = keras.Model(inputs=[input_], outputs=[output])


model_1.compile(loss=keras.losses.MeanSquaredError(), optimizer=keras.optimizers.SGD(learning_rate=1e-3))


history = model_1.fit(X_train, y_train, epochs=20, validation_data=(X_valid, y_valid), verbose=False)


mse_test = model_1.evaluate(X_test, y_test)
print(mse_test)





input_A = keras.layers.Input(shape=[5], name="wide_input")
input_B = keras.layers.Input(shape=[6], name="deep_input")
hidden1 = keras.layers.Dense(30, activation="relu")(input_B)
hidden2 = keras.layers.Dense(30, activation="relu")(hidden1)
concat = keras.layers.concatenate([input_A, hidden2])
output = keras.layers.Dense(1, name="output")(concat)


model_2 = keras.Model(inputs=[input_A, input_B], outputs=[output])


model_2.compile(loss='mse', optimizer=keras.optimizers.SGD(learning_rate=1e-3))


model_2.layers


X_train_A, X_train_B = X_train[:, :5], X_train[:, 2:]
X_valid_A, X_valid_B = X_valid[:, :5], X_valid[:, 2:]
X_test_A, X_test_B = X_test[:, :5], X_test[:, 2:]
X_new_A, X_new_B = X_test_A[:3], X_test_B[:3]

history = model_2.fit((X_train_A, X_train_B), y_train, epochs=20,
    validation_data=((X_valid_A, X_valid_B), y_valid), verbose=0)


mse_test = model_2.evaluate((X_test_A, X_test_B), y_test)
print(mse_test)


y_pred = model_2.predict((X_new_A, X_new_B))



model_2.evaluate((X_test_A, X_test_B), y_test)











class WideAndDeepModel(keras.Model):
    def __init__(self, units=30, activation="relu", **kwargs):
        super().__init__(**kwargs) # handles standard args (e.g., name)
        self.hidden1 = keras.layers.Dense(units, activation=activation)
        self.hidden2 = keras.layers.Dense(units, activation=activation)
        self.main_output = keras.layers.Dense(1)
        self.aux_output = keras.layers.Dense(1)
        
    def call(self, inputs):
        input_A, input_B = inputs
        hidden1 = self.hidden1(input_B)
        hidden2 = self.hidden2(hidden1)
        concat = keras.layers.concatenate([input_A, hidden2])
        main_output = self.main_output(concat)
        aux_output = self.aux_output(hidden2)
        return main_output, aux_output


model_3 = WideAndDeepModel()


model_3.compile(loss=["mse", "mse"], loss_weights=[0.9, 0.1], optimizer="sgd")


history = model_3.fit(
[X_train_A, X_train_B], [y_train, y_train], epochs=20,
validation_data=([X_valid_A, X_valid_B], [y_valid, y_valid]), verbose=0)


total_loss, main_loss, aux_loss = model_3.evaluate([X_test_A, X_test_B], [y_test, y_test])
print(f'total_loss: {total_loss} | main_loss: {main_loss} | aux_loss: {aux_loss}')





model_1.save('model_1.h5')


not_model_1 = keras.models.load_model("model_1.h5")





model_6 = keras.models.Sequential([
    keras.layers.InputLayer(name="Input layer", shape=X_train.shape[1:]),
    keras.layers.Flatten(),
    keras.layers.Dense(300, activation='relu'),
    keras.layers.Dense(100, activation='relu'),
    keras.layers.Dense(1, activation='relu')
])


model_6.compile(loss='mse', optimizer=keras.optimizers.SGD(learning_rate=1e-4))


checkpoint_cb = keras.callbacks.ModelCheckpoint("my_keras_model.keras",
    save_best_only=True)


early_stopping_cb = keras.callbacks.EarlyStopping(patience=10,
    restore_best_weights=True)


class PrintValTrainRatioCallback(keras.callbacks.Callback):
    def on_epoch_end(self, epoch, logs):
        print("\nval/train: {:.2f}".format(logs["val_loss"] / logs["loss"]))


history = model_6.fit(X_train, y_train, epochs=100, validation_data=(X_valid, y_valid), 
                    verbose=2, callbacks=[checkpoint_cb])


mse_test = model_6.evaluate(X_test, y_test)
X_new = X_test[:3]
print(mse_test)
y_pred = model_6.predict(X_new)








import os
root_logdir = os.path.join(os.curdir, "my_logs")
def get_run_logdir():
    import time
    run_id = time.strftime("run_%Y_%m_%d-%H_%M_%S")
    return os.path.join(root_logdir, run_id)

run_logdir = get_run_logdir()
run_logdir


tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)

history = model_6.fit(X_train, y_train, epochs=30,
    validation_data=(X_valid, y_valid), verbose=0,
    callbacks=[tensorboard_cb])


!tree my_logs/


# !tensorboard --logdir=./my_logs --port=6006








!pip3 install scikeras


def build_model(n_hidden=1, n_neurons=30, learning_rate=0.01, input_shape=[8]):
    model = keras.models.Sequential()
    model.add(keras.layers.InputLayer(input_shape=input_shape))
    for layer in range(n_hidden):
        model.add(keras.layers.Dense(n_neurons, activation="relu"))
    model.add(keras.layers.Dense(1))
    optimizer = keras.optimizers.SGD(learning_rate=learning_rate)
    model.compile(loss="mse", optimizer=optimizer)
    return model


from scikeras.wrappers import KerasRegressor
keras_reg = KerasRegressor(model=build_model)


keras_reg.fit(X_train, y_train, epochs=100,
    validation_data=(X_valid, y_valid),
    callbacks=[keras.callbacks.EarlyStopping(patience=10)])

mse_test = keras_reg.score(X_test, y_test)
y_pred = keras_reg.predict(X_new)


from scipy.stats import  reciprocal
from sklearn.model_selection import RandomizedSearchCV

param_distribs = {
    "model__n_hidden": [0, 1, 2, 3],
    "model__n_neurons": np.arange(1, 100),
    "model__learning_rate": reciprocal(3e-4, 3e-2)
}
# don't directly pass "learning_rate": reciprocal(3e-4, 3e-2) or such since scikeras uses a wrapper it should be passed to model__learning_rate": reciprocal(3e-4, 3e-2)


rnd_search_cv = RandomizedSearchCV(keras_reg, param_distribs, n_iter=10, cv=3)


rnd_search_cv.fit(X_train, y_train, epochs=100,
                    validation_data=(X_valid, y_valid),
                    callbacks=[keras.callbacks.EarlyStopping(patience=10)])


rnd_search_cv.best_params_


rnd_search_cv.best_score_


model = build_model(n_hidden==2, n_neurons=38, learning_rate=0.0070966866561813285)


from scikeras.wrappers import KerasRegressor
keras_reg = KerasRegressor(
    model=build_model(n_hidden=2, n_neurons=38, learning_rate=0.0070966866561813285)
)


# Refit it manually (this will build and train the model)
keras_reg.fit(X_train, y_train,
    epochs=100,
    validation_data=(X_valid, y_valid),
    callbacks=[keras.callbacks.EarlyStopping(patience=10)]
)


# Now the internal model is available
model = keras_reg.model_
model


# Save the model
model.save("best_model.keras")















